


# Your code here
%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu



import data
import matplotlib.pyplot as plt

plt.figure(figsize =(3,3))
plt.scatter(data.x_values, data.y_values, c = data.color_values)
plt.show()

import torch

model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), #input x, y
    torch.nn.Sigmoid(),
    torch.nn.Sigmoid(),
    torch.nn.Linear(50, 1)) #output z

loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)

for t in range(1000):
    y_pred = model(data.x)
    loss = loss_fn(y_pred, data.y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")





# Your comment here 
# Artificial intellegnce refers to a broad field of "intelegent machines". Machine learning is a subfield of ai where a system learns 
# from data models. Deep learning is a type of machine learning that uses nueral networks to proccess data in layers. 





# Your comment here
# A neural network is a system of nodes (neurons) aranged in layers that takes in an input and layer by layer predicts the output. 





# Your comment here
# the 2 and 50 represent the x and y inputs.





# Your comment here
# The activation function (torch.nn) allows you to make graphs.





# Your comment here
# The loss function determines how much is wrong/missing from the data. 





# Your comment here
 #The zero_grad function resets of the tensors of the optimized tensor which must be done before calling loss.backward becuase the
# function is reevaluated multiple times. 





# Your code here
    torch.nn.Linear(2, 50), #input x, y
    torch.nn.Linear(2, 30), #input x, y





# Your comment here
# 50 neruons : 0.2191433310508728
# 30 neurons : 0.5364993810653687





# Your comment here
# Sigmoid : 0.2191433310508728
# ReLU : 0.4518009126186371





# Your comment here
# Tanh : 0.15376290678977966





# Your comment here 
# ReLU has the greatest loss after the 1000th ittereation and Tanh had the least loss. Sigmoid was inbetween. 





# Your comment here
# 1e-3 : 0.2191433310508728
# 1e-2 : 0.052082035690546036





# Your code here
model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), 
    torch.nn.Sigmoid(),
    torch.nn.Sigmoid(),
    torch.nn.Linear(50, 1))





# Your code here
import new_data





# Your code here
plt.figure(figsize =(3,3))
plt.scatter(new_data.x_values, new_data.y_values, c = new_data.color_values)
plt.show()

import torch

model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), #input x, y
    torch.nn.Sigmoid(),
    torch.nn.Linear(50, 1)) #output z

loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)

for t in range(1000):
    y_pred = model(new_data.x)
    loss = loss_fn(y_pred, new_data.y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")

new_x = torch.linspace(-10, 10, 100)
poor_grid = torch.stack(torch.meshgrid((new_x, new_x)), dim = 2)

output1 = model(poor_grid).detach()
plt.imshow(output1, extent=(-10, 10, -10, 10)) 
plt.show()









# Your code here
# trianing the model better
plt.figure(figsize =(3,3))
better_model = plt.scatter(new_data.x_values, new_data.y_values, c = new_data.color_values)
plt.show()

import torch

model = torch.nn.Sequential(
    torch.nn.Linear(2, 60), #input x, y
    torch.nn.ReLU(),
    torch.nn.ReLU(),
    torch.nn.ReLU(),
    torch.nn.ReLU(),
    torch.nn.Linear(60, 1)) #output z

loss_fn = torch.nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)

for t in range(5000):
    y_pred = model(new_data.x)
    loss = loss_fn(y_pred, new_data.y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")



# better model
new_x = torch.linspace(-10, 10, 100)
well_grid = torch.stack(torch.meshgrid((new_x, new_x)), dim = 2)

output2 = model(well_grid).detach()
plt.imshow(output2, extent=(-10, 10, -10, 10))
plt.show()
#actual pattern
plt.imshow(data.pattern)



fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2)
ax1.imshow(data.pattern)
ax1.set_title('hidden data')
ax2.scatter(new_data.x_values, new_data.y_values, c = new_data.color_values)
ax2.set_title('data')
ax3.imshow(output1, extent=(-10, 10, -10, 10))
ax3.set_title('poor trained model')
ax4.imshow(output2, extent=(-10, 10, -10, 10)) 
ax4.set_title('well trained model')





# Your code here



